{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d7cca2f-33a9-4087-8796-3cef3bfd1219",
   "metadata": {},
   "source": [
    "# 1.概述\n",
    "\n",
    "分词算法是NLP领域内的基本任务，大多数NLP任务都会涉及到分词算法的使用。一般说到的tokenizer其实就是分词器。  \n",
    "没有一种分词算法是能够解决所有场景的问题的，所以分词算法也是在不断优化的。之所以**没有一个大统一的方法**去适用所有的场景是因为大多分词算法都需要权衡以下三个问题：  \n",
    "**1）OOV(Out-of-Vocabulary)问题：**遇到未登录词如何解决，或者说如何分词能避免出现未登录词的情况；  \n",
    "**2）分词粒度：**粒度粗的分法能带来更具体的语义以及减少序列长度，粒度细的分法能尽可能避免OOV问题以及降低词汇表大小，分词算法如何在分词粒度层面进行选择；  \n",
    "**3）歧义问题：**分词没有一定的标准，在不同的场景对分词粒度的要求也不同；  \n",
    "从思路上，我们可以以两个维度去梳理现有的分词方法：  \n",
    "**1）按分词方法划分：**基于词典匹配的分词方法、基于统计模型的分词方法、基于深度学习的分词方法  \n",
    "**2）按分词粒度维度：**word、subword、char  \n",
    "\n",
    "**中英文分词**\n",
    "https://easyai.tech/ai-definition/tokenization/\n",
    "\n",
    "**分词方法:** https://zhuanlan.zhihu.com/p/620603105  \n",
    "**1) char-based：** 字符分词法，适合中文,参考：https://arxiv.org/pdf/1905.05526  \n",
    "**2) word-based：** 单词分词法，适合英文  \n",
    "**3) subword（子词）：** char-based + word-based，词根（英文）或词组（中文）分词法\n",
    "- A. Byte Pair Encoding （BPE）  https://github.com/rsennrich/subword-nmt  \n",
    "  \n",
    "  - **minBPE**：https://github.com/karpathy/minbpe  \n",
    "    [作者视频解说](https://www.bilibili.com/video/BV1BH4y1N7JA/?spm_id_from=333.337.search-card.all.click&vd_source=6616c1ef2d5d1b0f463724e69d204363)  \n",
    "    [国内视频解说](https://www.bilibili.com/video/BV12x4y1t75q/?spm_id_from=333.337.search-card.all.click&vd_source=6616c1ef2d5d1b0f463724e69d204363)  \n",
    "  - 因其能够__有效处理OOV问题__和保持词根词缀的完整性，而被广泛应用于大型语言模型\n",
    "  - 近期，BPE技术已经发展成为**Byte-level BPE（BBPE）**\n",
    "- B. Unigram\n",
    "- C. WordPiece  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a89a0fe-4181-46e6-a658-7b67fa0b1508",
   "metadata": {},
   "source": [
    "## 1.1 minBPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3237a4d0-ceac-4491-9a10-e7974274ffbd",
   "metadata": {},
   "source": [
    "**minbpe/base.py:**   \n",
    "Implements the **Tokenizer class**, which is the base class. It contains the **train, encode, and decode** stubs, **save/load** functionality, and there are also a few common utility functions. This class is not meant to be used directly, but rather to be inherited from.  \n",
    "**minbpe/basic.py:**  \n",
    "**Implements the BasicTokenizer**, the simplest implementation of the BPE algorithm that runs directly on text.  \n",
    "**minbpe/regex.py:**  \n",
    "Implements the RegexTokenizer that further **splits the input text by a regex pattern**, which is a preprocessing stage that splits up the input text by categories (think: letters, numbers, punctuation) before tokenization. This ensures that no merges will happen across category boundaries. This was introduced in the GPT-2 paper and continues to be in use as of GPT-4. This class also handles special tokens, if any.  \n",
    "**minbpe/gpt4.py:**  \n",
    "Implements the **GPT4Tokenizer**. This class is a light wrapper around the RegexTokenizer (2, above) that **exactly reproduces the tokenization of GPT-4 in the [tiktoken](https://github.com/openai/tiktoken) library**. The wrapping handles some details around recovering the exact merges in the tokenizer, and the handling of some unfortunate (and likely historical?) 1-byte token permutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af4f1619-5a9a-46fb-87ba-3ad4e9689698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'minbpe' already exists and is not an empty directory.\n",
      "Requirement already satisfied: pip in /home/ryan/Libs/miniconda3/envs/self/lib/python3.12/site-packages (24.2)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Using cached regex-2024.7.24-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/ryan/Libs/miniconda3/envs/self/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ryan/Libs/miniconda3/envs/self/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ryan/Libs/miniconda3/envs/self/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ryan/Libs/miniconda3/envs/self/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ryan/Libs/miniconda3/envs/self/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
      "Downloading tiktoken-0.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached regex-2024.7.24-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (790 kB)\n",
      "Installing collected packages: regex, tiktoken\n",
      "Successfully installed regex-2024.7.24 tiktoken-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/karpathy/minbpe\n",
    "!pip install --upgrade pip\n",
    "try:\n",
    "    import tiktoken\n",
    "except ImportError:\n",
    "    !pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4dbc21-b63b-4f98-a141-a2fceb1cde6d",
   "metadata": {},
   "source": [
    "### 1.1.1 quick start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d878f39-ab6d-4a6e-8c1a-0e3ef6b43f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[258, 100, 258, 97, 99]\n",
      "aaabdaaabac\n"
     ]
    }
   ],
   "source": [
    "from minbpe.minbpe import BasicTokenizer\n",
    "tokenizer = BasicTokenizer()\n",
    "text = \"aaabdaaabac\"\n",
    "tokenizer.train(text, 256 + 3) # 256 are the byte tokens, then do 3 merges\n",
    "print(tokenizer.encode(text))\n",
    "# [258, 100, 258, 97, 99]\n",
    "print(tokenizer.decode([258, 100, 258, 97, 99]))\n",
    "# aaabdaaabac\n",
    "tokenizer.save(\"toy\")\n",
    "# writes two files: toy.model (for loading) and toy.vocab (for viewing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd27472d-3341-47a9-9442-9fbf06b3f94b",
   "metadata": {},
   "source": [
    "### 1.1.2 inference: GPT-4 comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
